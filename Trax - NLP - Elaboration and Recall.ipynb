{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About This Notebook\n",
    "\n",
    "This is a playground notebook of sorts where I can try to implement a lot of the code from the assignments from scratch.\n",
    "\n",
    "The assignments are helpful but there are too many hints and it's too easy for someone to work through them without truely understanding how transformers, attention, etc. works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms\n",
    "\n",
    "* cross attention\n",
    "* causal attention\n",
    "* scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dp_attention(q, k, v, m, embed_dims, vocab_size):\n",
    "    '''\n",
    "    k - keys [batch, seq_len_in, embed_dims]\n",
    "    q - query [batch, seq_len_out, embed_dims]\n",
    "    v - values [batch, seq_len_in, embed_dims]\n",
    "    m - mask [batch, seq_len_out, seq_len_in]\n",
    "    '''\n",
    "    kt  = jnp.transpose(k)\n",
    "    qkt = jnp.dot(q, kt)\n",
    "    mqk = jnp.where(m==True, qtk, jnp.full_like(qkt, -1e9)) #add in the mask\n",
    "    w   = softmax(mqk/sqrt(embed_dims))\n",
    "    \n",
    "    return jnp.dot(w,v)\n",
    "    \n",
    "    #Note: it's a little unclear to me how I can use trax layers with jax.numpy operations. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax with no division or multiplication to avoid underflow and overflow. \n",
    "def softmax(x):\n",
    "    d = trax.fastmath.logsumexp(x, axis=-1, keepdims=True)\n",
    "    p = jnp.exp(x - d)\n",
    "    return p #array of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention\n",
    "\n",
    "a.k.a masked-multi head attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    pass\n",
    "\n",
    "def dot_product_self_attention():\n",
    "    pass\n",
    "\n",
    "def compute_attention_output_closure():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CausalAttention():\n",
    "    \"\"\"Transformer-style multi-headed causal attention.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder / Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nmt_encoder(vocab_size, embed_dims, lstm_hidden, num_lstms):\n",
    "    '''\n",
    "    Encodes the input token sequences before passing it along to attention and then on to the decoder.\n",
    "    '''\n",
    "    input_encoder = tl.Serial(\n",
    "        tl.Embedding(vocab_size, embed_dims),\n",
    "        [tl.LSTM(lstm_hidden) for _ in range(num_lstms) ],\n",
    "    )\n",
    "    \n",
    "    return input_encoder\n",
    "    \n",
    "def pre_attention_decoder(target_vocab_size, embed_dims, lstm_hidden):\n",
    "    '''\n",
    "    \"Decodes\" the target target token sequences. The decoder hidden state will be used as queries in the attention layers. \n",
    "    - This is just the first decoder in this model. A second decoder runs after the attention layer. \n",
    "    - Shifts the supplied token sequences right before running the layers. \n",
    "    - Note: only a signle LSTM was used for the decoder vs. many for the encoder. Not really sure why. \n",
    "    '''\n",
    "    \n",
    "    pa_target_decoder = tl.Serial(\n",
    "        tl.ShiftRight(),\n",
    "        tl.Embedding(target_vocab_size, embed_dims),\n",
    "        tl.LSTM(lstm_hidden),\n",
    "    )\n",
    "    return pa_target_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention helper functions\n",
    "\n",
    "def prepare_qkv_from_encoder_decoder(encoder_activations, decoder_activations, input_token_seq, attention_heads=1):\n",
    "'''\n",
    "   Super light level of indirection that maps activations to QKV in attention.\n",
    "'''\n",
    "    queries = decoder_activations\n",
    "    keys    = encoder_activations\n",
    "    values  = encoder_activations\n",
    "    \n",
    "    (batch_size, seq_len) = input_token_seq.shape \n",
    "    (_, decoder_len)      = decoder_activations.shape\n",
    "    \n",
    "    # if token is padding then mask = 0, otherwise 1\n",
    "    mask  = jnp.where(input_token_seq == 0, 0, 1)\n",
    "    \n",
    "    #mask dims transform:  batch_size x seq_len -> batch_size, attention_heads, decoder_length, encoder_length\n",
    "    \n",
    "    #add dimensions\n",
    "    mask = jnp.reshape(mask, (batch_size, 1, 1, seq_len))\n",
    "    \n",
    "    #QUESTION: is the seq_len and the encoder length the same? Maybe just the dims change. \n",
    "    \n",
    "    # adding in this way causes broadcast / dim expansion\n",
    "    mask = mask + jnp.zeros((1, attention_heads, decoder_len, 1))\n",
    "    \n",
    "    return (queries, keys, values, mask)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trax-ml",
   "language": "python",
   "name": "trax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
