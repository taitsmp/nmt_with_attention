{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About This Notebook\n",
    "\n",
    "This is a playground notebook of sorts where I can try to implement a lot of the code from the assignments from scratch.\n",
    "\n",
    "The assignments are helpful but there are too many hints and it's too easy for someone to work through them without truely understanding how transformers, attention, etc. works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms\n",
    "\n",
    "* cross attention\n",
    "* causal attention\n",
    "* scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dp_attention(q, k, v, m, embed_dims=None):\n",
    "    '''\n",
    "    k - keys [batch, seq_len_in, embed_dims]\n",
    "    q - query [batch, seq_len_out, embed_dims]\n",
    "    v - values [batch, seq_len_in, embed_dims]\n",
    "    m - mask [batch, seq_len_out, seq_len_in]\n",
    "    '''\n",
    "    \n",
    "    if embed_dims is None:\n",
    "        embed_dims = k.shape[-1]\n",
    "    \n",
    "    kt  = jnp.transpose(k)\n",
    "    qkt = jnp.dot(q, kt)\n",
    "    mqk = jnp.where(m==True, qtk, jnp.full_like(qkt, -1e9)) #add in the mask\n",
    "    w   = softmax(mqk/sqrt(embed_dims))\n",
    "    \n",
    "    return jnp.dot(w,v)\n",
    "    \n",
    "    #Note: it's a little unclear to me how I can use trax layers with jax.numpy operations. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax with no division or multiplication to avoid underflow and overflow. \n",
    "def softmax(x):\n",
    "    d = trax.fastmath.logsumexp(x, axis=-1, keepdims=True)\n",
    "    p = jnp.exp(x - d)\n",
    "    return p #array of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention\n",
    "\n",
    "a.k.a masked-multi head attention\n",
    "\n",
    "See assignment 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Breaks down the larger \"embedding dimension\" into n_heads by d_head.  \n",
    "\n",
    "def compute_attention_heads_closure(n_heads, head_embed_dims):\n",
    "    \n",
    "    def compute_attention_heads(x):\n",
    "        \n",
    "        batch_sz, seq_len, embed_dims = x.shape\n",
    "        \n",
    "        y = jnp.reshape(x, (batch_sz, seq_len, n_heads, head_embed_dims))\n",
    "        z = jnp.transpose(y, (0, 2, 1, 3))\n",
    "        \n",
    "        #not totally sure why we decide to combine the batch size and number of heads. \n",
    "        return jnp.reshape(z, (batch_sz * n_heads, seq_len, head_embed_dims))\n",
    "          \n",
    "    \n",
    "    return compute_attention_heads\n",
    "\n",
    "def dot_product_self_attention(q, k, v):\n",
    "    \n",
    "    #mast the top right of the matrix.\n",
    "    \n",
    "    m_rows = k.shape[1]\n",
    "    m_cols = q.shape[1]\n",
    "    \n",
    "    #why use a 1 here? \n",
    "    mask = jnp.ones((1, m_rows, m_cols), dtype=jnp.bool_)\n",
    "    mask = jnp.tril(mask, k=0 )\n",
    "    \n",
    "    return scaled_dp_attention(q, k, v, mask)\n",
    "    \n",
    "\n",
    "# this function performs the opposite action of compute_attention_heads_closure\n",
    "def compute_attention_output_closure(n_heads, head_embed_dims):\n",
    "    \n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "        \n",
    "        batch_times_heads, seq_len, embed_dims = x.shape\n",
    "        batch_sz = batch_times_heads // n_heads\n",
    "        \n",
    "        y = jnp.reshape(x, (batch_sz, n_heads, seq_len, head_embed_dims))\n",
    "        # to shape (batch_sz, seq_len, n_heads, head_embed_dims)\n",
    "        z = jnp.transpose(y, (0, 2, 1, 3))\n",
    "        \n",
    "        return jnp.reshape(z, (batch_sz, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CausalAttention():\n",
    "    \"\"\"Transformer-style multi-headed causal attention.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder / Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nmt_encoder(vocab_size, embed_dims, lstm_hidden, num_lstms):\n",
    "    '''\n",
    "    Encodes the input token sequences before passing it along to attention and then on to the decoder.\n",
    "    '''\n",
    "    input_encoder = tl.Serial(\n",
    "        tl.Embedding(vocab_size, embed_dims),\n",
    "        [tl.LSTM(lstm_hidden) for _ in range(num_lstms) ],\n",
    "    )\n",
    "    \n",
    "    return input_encoder\n",
    "    \n",
    "def pre_attention_decoder(target_vocab_size, embed_dims, lstm_hidden):\n",
    "    '''\n",
    "    Decodes\" the target target token sequences. The decoder hidden state will be used as queries in the attention layers. \n",
    "    - This is just the first decoder in this model. A second decoder runs after the attention layer. \n",
    "    - Shifts the supplied token sequences right before running the layers. \n",
    "    - Note: only a single LSTM was used for the decoder vs. many for the encoder. Not really sure why. \n",
    "    '''\n",
    "    \n",
    "    pa_target_decoder = tl.Serial(\n",
    "        tl.ShiftRight(),\n",
    "        tl.Embedding(target_vocab_size, embed_dims),\n",
    "        tl.LSTM(lstm_hidden),\n",
    "    )\n",
    "    return pa_target_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention helper functions\n",
    "\n",
    "def prepare_qkv_from_encoder_decoder(encoder_activations, decoder_activations, input_token_seq):\n",
    "    '''\n",
    "    Preparing inputs for attention layer from activations of encoder and decoder hidden states.\n",
    "    Super light level of indirection that maps activations to QKV in attention.\n",
    "    '''\n",
    "    attention_heads=1\n",
    "    \n",
    "    queries = decoder_activations\n",
    "    keys    = encoder_activations\n",
    "    values  = encoder_activations\n",
    "    \n",
    "    (batch_size, seq_len) = input_token_seq.shape \n",
    "    (_, _, dec_embed_dim)   = decoder_activations.shape \n",
    "    \n",
    "    # if token is padding then mask = 0, otherwise 1\n",
    "    mask  = jnp.where(input_token_seq == 0, 0, 1)\n",
    "    \n",
    "    #mask dims transform:  batch_size x seq_len -> batch_size, attention_heads, decoder_length, encoder_length\n",
    "    \n",
    "    #add dimensions\n",
    "    mask = jnp.reshape(mask, (batch_size, 1, 1, seq_len))\n",
    "    \n",
    "    #QUESTION: is the seq_len and the encoder length the same? Maybe just the dims change. \n",
    "    \n",
    "    # adding in this way causes broadcast / dim expansion\n",
    "    mask = mask + jnp.zeros((1, attention_heads, decoder_embed_dim, 1))\n",
    "    \n",
    "    return (queries, keys, values, mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMTAttn(input_vocab_size=33300,\n",
    "            target_vocab_size=33300,\n",
    "            d_model=1024,\n",
    "            n_encoder_layers=2,\n",
    "            n_decoder_layers=2,\n",
    "            n_attention_heads=4,\n",
    "            attention_dropout=0.0,\n",
    "            mode='train'):\n",
    "    \n",
    "    tl.Serial([\n",
    "        tl.Select([0,1,0,1]),\n",
    "        tl.Parallel(\n",
    "            nmt_encoder(input_vocab_size, d_model, d_model, n_encoder_layers),\n",
    "            pre_attention_decoder(target_vocab_size, d_model, d_model),\n",
    "        ),\n",
    "        #takes three inputs. Leaves one on stack...\n",
    "        tl.Fn('PrepareAttentionInput', prepare_qkv_from_encoder_decoder ,n_out=4),\n",
    "        \n",
    "        #https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual\n",
    "        #TODO: Can you understand the Stack and Residual better? \n",
    "        tl.Residual(\n",
    "            tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)\n",
    "        ),\n",
    "        #this just drops the mask. \n",
    "        tl.Select([0,2]),\n",
    "        [tl.LSTM(d_model) for _ in range(n_decoder_layers)], #do you need to use d_model here? \n",
    "        tl.Dense(target_vocab_size),\n",
    "        tl.LogSoftmax()\n",
    "    ])\n",
    "\n",
    "#TODO: test me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"NMTModel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Model Once\n",
    "\n",
    "Test the model to make sure you understand the inputs and outputs and to make sure there are no errors in how you coded it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
    "    \n",
    "    \"\"\"Returns the index of the next token.\n",
    "\n",
    "    Args:\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
    "        cur_output_tokens (list): tokenized representation of previously translated words\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        int: index of the next token in the translated sentence\n",
    "        float: log probability of the next symbol\n",
    "    \"\"\"\n",
    "    \n",
    "    expanded_op_tokens = pad_and_expand(cur_output_tokens)\n",
    "    \n",
    "    output, targs = NMTAttn((input_tokens, expanded_op_tokens))\n",
    "    \n",
    "    #output dims are [batch, seq_len+1, vocab_size] - grab the last prob array\n",
    "    log_probs = output[0, len(cur_output_tokens), :]\n",
    "    \n",
    "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
    "    \n",
    "    return symbol\n",
    "    \n",
    "\n",
    "def pad_and_expand(cur_output_tokens):\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    token_length = len(cur_output_tokens)\n",
    "    \n",
    "    # calculate next power of 2 for padding length \n",
    "    padded_length = 2**math.ceil(math.log2(token_length+1))\n",
    "\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
    "    \n",
    "    return padded_with_batch\n",
    "\n",
    "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
    "    pass\n",
    "    #write me? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = NMTAttn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8c6745b91a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokens_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m17332\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m172\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m207\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnext_de_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m18477\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_de_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-1d34c4b915d4>\u001b[0m in \u001b[0;36mnext_symbol\u001b[0;34m(NMTAttn, input_tokens, cur_output_tokens, temperature)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mexpanded_op_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_and_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_output_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMTAttn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_op_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#output dims are [batch, seq_len+1, vocab_size] - grab the last prob array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens_en = np.array([[17332, 140, 172, 207, 1]])\n",
    "next_de_tokens = next_symbol(the_model, tokens_en, [18477], 0.0)\n",
    "print(next_de_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "c10 = jnp.zeros((1, 10,))\n",
    "r10 = jnp.zeros((10, 1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr10 = c10 + r10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trax Practice\n",
    "\n",
    "* TODO: read the [Layers Intro](https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html)\n",
    "* Try to create a CreateFirst where m is the first thing returned. What happens\n",
    "* Download the ungrade trax lab from Coursera NLP C4 W1 and experiment with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a similar network to what we have above but let's make it simple and deterministic\n",
    "\n",
    "#TODO: could you change this to work with jax DeviceArrays just to see how it works? \n",
    "\n",
    "def Addition():\n",
    "    layer_name = \"Addition\"  # don't forget to give your custom layer a name to identify\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x, y):\n",
    "        return jnp.add(x,y)\n",
    "\n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "def Multiplication():\n",
    "    layer_name = (\n",
    "        \"Multiplication\"  # don't forget to give your custom layer a name to identify\n",
    "    )\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x, y):\n",
    "        return jnp.multiply(x,y) #element-wise\n",
    "    \n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "# Simple layer to create one new argument. Similar stack effect to prepare_qkv_from_encoder_decoder\n",
    "def CreateOne():\n",
    "    layer_name = (\n",
    "        \"CreateOne\"\n",
    "    )\n",
    "\n",
    "    def func(a,b,c):\n",
    "        m = jnp.array([1000.0, 2000.0])\n",
    "        return (a, b, c, m)\n",
    "    \n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "def Divide():\n",
    "    layer_name = (\n",
    "        \"Divide\"\n",
    "    )\n",
    "\n",
    "    def func(a,b):\n",
    "        return jnp.divide(a,b)\n",
    "    \n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "def Noop():\n",
    "    layer_name = (\n",
    "        \"Noop\"\n",
    "    )\n",
    "\n",
    "    def func(a,b):\n",
    "        return (a,b)\n",
    "    \n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "simple = tl.Serial([\n",
    "    tl.Select([0,1,0,1,0,1,0,1,0,1]),\n",
    "    CreateOne()\n",
    "    \n",
    "])\n",
    "\n",
    "topOfStack = tl.Serial([\n",
    "    tl.Select([0,1,0,1]),\n",
    "    CreateOne(),\n",
    "    tl.Select([0], n_in=2) #wanted to set this to 4 but I get an error with any number about 2\n",
    "])\n",
    "\n",
    "invert = tl.Serial([\n",
    "    tl.Select([1,0]), # 0 references top of the stack. first position is top of new stack.  \n",
    "])\n",
    "\n",
    "invert2 = tl.Serial([\n",
    "    tl.Select([1,0]), \n",
    "    Noop(),\n",
    "])\n",
    "\n",
    "simple.n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([1., 2.], dtype=float32),\n",
       " DeviceArray([100., 200.], dtype=float32),\n",
       " DeviceArray([1., 2.], dtype=float32),\n",
       " DeviceArray([1000., 2000.], dtype=float32),\n",
       " DeviceArray([100., 200.], dtype=float32),\n",
       " DeviceArray([1., 2.], dtype=float32),\n",
       " DeviceArray([100., 200.], dtype=float32),\n",
       " DeviceArray([1., 2.], dtype=float32),\n",
       " DeviceArray([100., 200.], dtype=float32),\n",
       " DeviceArray([1., 2.], dtype=float32),\n",
       " DeviceArray([100., 200.], dtype=float32))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_tok = jnp.array([1.0, 2.0])\n",
    "out_tok = jnp.array([100.0, 200.0])\n",
    "\n",
    "simple((in_tok, out_tok))\n",
    "\n",
    "\n",
    "#It appears that when you call serial\n",
    "# - The first argument you give it goes on the top of the stack. \n",
    "# - The last argument goes on the bottom.\n",
    "\n",
    "\n",
    "#note: in the returned output, the first x values are the ordered returned items from the last function.\n",
    "#in this case it's a,b,c,m in spots 0-3 of the returned tuple below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([1., 2.], dtype=float32),\n",
       " DeviceArray([1., 2.], dtype=float32),\n",
       " DeviceArray([1000., 2000.], dtype=float32),\n",
       " DeviceArray([100., 200.], dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topOfStack((in_tok, out_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([100., 200.], dtype=float32),\n",
       " DeviceArray([1., 2.], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invert((in_tok, out_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([100., 200.], dtype=float32),\n",
       " DeviceArray([1., 2.], dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this implies \n",
    "# - the first argument to the function is the top of the stack. \n",
    "# - the first returned value from a function goes in the first position on a stack. \n",
    "invert2((in_tok, out_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stack rules\n",
    "\n",
    "* first argument to layer is top of stack\n",
    "* first item in tuple of layer goes on top of stack.\n",
    "* 0 in `tl.Select` refers to top of stack\n",
    "* first position in input array to `tl.Select` is the top of the stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trax-ml",
   "language": "python",
   "name": "trax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
